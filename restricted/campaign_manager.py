#!/usr/bin/env python3
"""
Campaign Management Utility for SBC Summit 2025
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞–º–ø–∞–Ω–∏–π —Å–æ–æ–±—â–µ–Ω–∏–π

–§—É–Ω–∫—Ü–∏–∏:
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–º–ø–∞–Ω–∏–π
- –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ CSV –¥–∞–Ω–Ω—ã–º–∏
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
import argparse

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from campaign_config import *


class CampaignManager:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–º–ø–∞–Ω–∏—è–º–∏"""

    def __init__(self):
        self.data_dir = Path("restricted/data")
        self.csv_file = self.data_dir / "SBC - Attendees.csv"
        self.campaign_log_file = self.data_dir / "first_wave_campaign_log.json"

    def load_campaign_log(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–≥ –∫–∞–º–ø–∞–Ω–∏–π"""
        if self.campaign_log_file.exists():
            try:
                with open(self.campaign_log_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–≥–∞: {e}")
        return {"campaigns": [], "contacted_users": []}

    def analyze_csv_data(self) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º CSV —Ñ–∞–π–ª–µ"""
        if not self.csv_file.exists():
            return {"error": "CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}

        try:
            df = pd.read_csv(self.csv_file)

            analysis = {
                "total_contacts": len(df),
                "contacted_count": len(df[df["connected"] == "Sent"]),
                "pending_contacts": len(
                    df[(df["connected"].isna()) | (df["connected"] != "Sent")]
                ),
                "with_responses": len(df[df["connected"] == "Sent Answer"]),
                "gaming_verticals": df["gaming_vertical"]
                .value_counts()
                .to_dict(),
                "top_companies": df["company_name"]
                .value_counts()
                .head(10)
                .to_dict(),
                "positions_distribution": df["position"]
                .value_counts()
                .head(15)
                .to_dict(),
                "countries": (
                    df["country"].value_counts().head(10).to_dict()
                    if "country" in df.columns
                    else {}
                ),
                "contact_quality": {
                    "with_phone": (
                        len(df[df["other_contacts"].notna()])
                        if "other_contacts" in df.columns
                        else 0
                    ),
                    "with_email": (
                        len(
                            df[
                                df["other_contacts"].str.contains(
                                    "@", na=False
                                )
                            ]
                        )
                        if "other_contacts" in df.columns
                        else 0
                    ),
                },
            }

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            analysis["data_quality"] = {
                "complete_profiles": len(
                    df.dropna(subset=["full_name", "user_id"])
                ),
                "missing_positions": len(df[df["position"].isna()]),
                "missing_companies": len(df[df["company_name"].isna()]),
                "missing_gaming_vertical": len(
                    df[df["gaming_vertical"].isna()]
                ),
            }

            return analysis

        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ CSV: {e}"}

    def get_campaign_statistics(self) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –∫–∞–º–ø–∞–Ω–∏—è–º"""
        campaign_log = self.load_campaign_log()

        stats = {
            "total_campaigns": len(campaign_log.get("campaigns", [])),
            "total_contacted": len(campaign_log.get("contacted_users", [])),
            "last_campaign": None,
            "campaign_history": [],
        }

        campaigns = campaign_log.get("campaigns", [])
        if campaigns:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
            sorted_campaigns = sorted(
                campaigns, key=lambda x: x.get("date", ""), reverse=True
            )
            stats["last_campaign"] = sorted_campaigns[0]
            stats["campaign_history"] = sorted_campaigns

        return stats

    def identify_target_audience(self, apply_filters: bool = True) -> Dict:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é –¥–ª—è –Ω–æ–≤–æ–π –∫–∞–º–ø–∞–Ω–∏–∏"""
        if not self.csv_file.exists():
            return {"error": "CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"}

        try:
            df = pd.read_csv(self.csv_file)
            campaign_log = self.load_campaign_log()
            contacted_users = set(campaign_log.get("contacted_users", []))

            # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–∞–Ω–∏–π
            companies_analysis = self._analyze_companies(df)

            # –ë–∞–∑–æ–≤–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            available = df[
                (df["connected"].isna()) | (df["connected"] != "Sent")
            ].copy()

            # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∏–∑ –ª–æ–≥–∞
            if contacted_users:
                available = available[
                    ~available["user_id"].isin(contacted_users)
                ]

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
            companies_with_answers = set()
            answered_df = df[
                df["connected"].str.contains("answer", case=False, na=False)
            ]
            if not answered_df.empty:
                companies_with_answers = set(
                    answered_df["company_name"].dropna().unique()
                )

            # –ò—Å–∫–ª—é—á–∞–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏–∑ –∫–æ–º–ø–∞–Ω–∏–π –≥–¥–µ —É–∂–µ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç—ã
            available = available[
                ~available["company_name"].isin(companies_with_answers)
            ]

            if apply_filters:
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

                # –§–∏–ª—å—Ç—Ä –ø–æ gaming_vertical
                online_mask = available["gaming_vertical"].str.contains(
                    "|".join(ONLINE_GAMING_KEYWORDS), case=False, na=False
                )
                empty_gaming_mask = (
                    available["gaming_vertical"].isna()
                    | (available["gaming_vertical"] == "")
                    | (available["gaming_vertical"].str.strip() == "")
                )
                land_mask = available["gaming_vertical"].str.contains(
                    "|".join(EXCLUDED_GAMING_KEYWORDS), case=False, na=False
                )

                available = available[
                    (online_mask | empty_gaming_mask) & ~land_mask
                ]

            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
            available = available.drop_duplicates(
                subset=["user_id"], keep="first"
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            available = available[
                available["user_id"].notna()
                & (available["user_id"] != "")
                & available["full_name"].notna()
                & (available["full_name"] != "")
            ]

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã
            priority_contacts = (
                available[
                    available["position"].str.contains(
                        "|".join(PRIORITY_POSITIONS), case=False, na=False
                    )
                ]
                if "position" in available.columns
                else pd.DataFrame()
            )

            result = {
                "total_available": len(available),
                "priority_contacts": len(priority_contacts),
                "regular_contacts": len(available) - len(priority_contacts),
                "gaming_verticals": available["gaming_vertical"]
                .value_counts()
                .to_dict(),
                "top_positions": (
                    available["position"].value_counts().head(10).to_dict()
                    if "position" in available.columns
                    else {}
                ),
                "top_companies": (
                    available["company_name"].value_counts().head(10).to_dict()
                    if "company_name" in available.columns
                    else {}
                ),
                "filters_applied": apply_filters,
                "contacted_previously": len(contacted_users),
                "companies_with_answers": len(companies_with_answers),
                "companies_analysis": companies_analysis,
            }

            return result

        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏: {e}"}

    def _analyze_companies(self, df: pd.DataFrame) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—É—Å –∫–æ–º–ø–∞–Ω–∏–π"""
        try:
            # –ö–æ–º–ø–∞–Ω–∏–∏ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ "answer" –≤ connected, case-insensitive)
            companies_answered = set()
            answered_df = df[
                df["connected"].str.contains("answer", case=False, na=False)
            ]
            if not answered_df.empty:
                companies_answered = set(
                    answered_df["company_name"].dropna().unique()
                )

            # –ö–æ–º–ø–∞–Ω–∏–∏ —Å –æ—Ç–ø—Ä–∞–≤–∫–æ–π –Ω–æ –±–µ–∑ –æ—Ç–≤–µ—Ç–∞
            companies_sent_no_answer = set()
            sent_df = df[df["connected"] == "Sent"]
            if not sent_df.empty:
                companies_sent = set(sent_df["company_name"].dropna().unique())
                companies_sent_no_answer = companies_sent - companies_answered

            # –ö–æ–º–ø–∞–Ω–∏–∏ –ø–æ–º–µ—á–µ–Ω–Ω—ã–µ –∫–∞–∫ "contacted with other worker"
            companies_marked = set()
            marked_df = df[df["connected"] == "contacted with other worker"]
            if not marked_df.empty:
                companies_marked = set(
                    marked_df["company_name"].dropna().unique()
                )

            # –ù–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
            processed_companies = (
                companies_answered
                | companies_sent_no_answer
                | companies_marked
            )
            all_companies = set(df["company_name"].dropna().unique())
            companies_unprocessed = all_companies - processed_companies

            return {
                "total_companies": len(all_companies),
                "companies_answered": len(companies_answered),
                "companies_sent_no_answer": len(companies_sent_no_answer),
                "companies_marked_contacted": len(companies_marked),
                "companies_unprocessed": len(companies_unprocessed),
                "companies_answered_list": list(companies_answered)[
                    :10
                ],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
                "companies_sent_no_answer_list": list(
                    companies_sent_no_answer
                )[:10],
            }
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–∞–Ω–∏–π: {e}"}

    def generate_campaign_report(self, detailed: bool = False) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –ø–æ –∫–∞–º–ø–∞–Ω–∏—è–º"""
        print("=" * 80)
        print("–û–¢–ß–ï–¢ –ü–û –ö–ê–ú–ü–ê–ù–ò–Ø–ú SBC SUMMIT 2025")
        print("=" * 80)

        # –ê–Ω–∞–ª–∏–∑ CSV –¥–∞–Ω–Ω—ã—Ö
        csv_analysis = self.analyze_csv_data()
        if "error" in csv_analysis:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {csv_analysis['error']}")
            return

        print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –î–ê–ù–ù–´–•")
        print(f"   –í—Å–µ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {csv_analysis['total_contacts']:,}")
        print(f"   –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {csv_analysis['contacted_count']:,}")
        print(f"   –ü–æ–ª—É—á–µ–Ω—ã –æ—Ç–≤–µ—Ç—ã: {csv_analysis['with_responses']:,}")
        print(f"   –û–∂–∏–¥–∞—é—Ç –æ—Ç–ø—Ä–∞–≤–∫–∏: {csv_analysis['pending_contacts']:,}")

        if csv_analysis["contacted_count"] > 0:
            response_rate = (
                csv_analysis["with_responses"]
                / csv_analysis["contacted_count"]
            ) * 100
            print(f"   –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç–≤–µ—Ç–æ–≤: {response_rate:.1f}%")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞–º–ø–∞–Ω–∏–π
        campaign_stats = self.get_campaign_statistics()
        print(f"\nüì¨ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ö–ê–ú–ü–ê–ù–ò–ô")
        print(f"   –ü—Ä–æ–≤–µ–¥–µ–Ω–æ –∫–∞–º–ø–∞–Ω–∏–π: {campaign_stats['total_campaigns']}")
        print(f"   –í—Å–µ–≥–æ —Å–≤—è–∑–∞–ª–∏—Å—å: {campaign_stats['total_contacted']:,}")

        if campaign_stats["last_campaign"]:
            last_campaign = campaign_stats["last_campaign"]
            print(f"   –ü–æ—Å–ª–µ–¥–Ω—è—è –∫–∞–º–ø–∞–Ω–∏—è: {last_campaign.get('date', 'N/A')}")
            print(
                f"   –ö–æ–Ω—Ç–∞–∫—Ç–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π: {last_campaign.get('contacts_sent', 0)}"
            )

        # –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
        target_analysis = self.identify_target_audience()
        if "error" not in target_analysis:
            print(f"\nüéØ –î–û–°–¢–£–ü–ù–ê–Ø –¶–ï–õ–ï–í–ê–Ø –ê–£–î–ò–¢–û–†–ò–Ø")
            print(
                f"   –î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏: {target_analysis['total_available']:,}"
            )
            print(
                f"   –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã: {target_analysis['priority_contacts']:,}"
            )
            print(
                f"   –û–±—ã—á–Ω—ã–µ –∫–æ–Ω—Ç–∞–∫—Ç—ã: {target_analysis['regular_contacts']:,}"
            )

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
            if "companies_analysis" in target_analysis:
                comp_stats = target_analysis["companies_analysis"]
                if "error" not in comp_stats:
                    print(f"\nüè¢ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–û–ú–ü–ê–ù–ò–Ø–ú")
                    print(
                        f"   –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {comp_stats['total_companies']:,}"
                    )
                    print(
                        f"   –° –æ—Ç–≤–µ—Ç–∞–º–∏ (–∏—Å–∫–ª—é—á–µ–Ω—ã): {comp_stats['companies_answered']:,}"
                    )
                    print(
                        f"   –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –±–µ–∑ –æ—Ç–≤–µ—Ç–∞: {comp_stats['companies_sent_no_answer']:,}"
                    )
                    print(
                        f"   –ü–æ–º–µ—á–µ–Ω—ã –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ: {comp_stats['companies_marked_contacted']:,}"
                    )
                    print(
                        f"   –î–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {comp_stats['companies_unprocessed']:,}"
                    )

                    if detailed and comp_stats["companies_answered"] > 0:
                        print(f"\nüìû –ö–û–ú–ü–ê–ù–ò–ò –° –û–¢–í–ï–¢–ê–ú–ò (–ø–µ—Ä–≤—ã–µ 10):")
                        for company in comp_stats["companies_answered_list"][
                            :10
                        ]:
                            print(f"   ‚Ä¢ {company}")

                    if detailed and comp_stats["companies_sent_no_answer"] > 0:
                        print(f"\n‚è≥ –ö–û–ú–ü–ê–ù–ò–ò –ë–ï–ó –û–¢–í–ï–¢–ê (–ø–µ—Ä–≤—ã–µ 10):")
                        for company in comp_stats[
                            "companies_sent_no_answer_list"
                        ][:10]:
                            print(f"   ‚Ä¢ {company}")

        # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if detailed:
            print(f"\nüè¢ –¢–û–ü –ö–û–ú–ü–ê–ù–ò–ò ({len(csv_analysis['top_companies'])})")
            for company, count in list(csv_analysis["top_companies"].items())[
                :10
            ]:
                print(f"   {company}: {count}")

            print(
                f"\nüíº –¢–û–ü –ü–û–ó–ò–¶–ò–ò ({len(csv_analysis['positions_distribution'])})"
            )
            for position, count in list(
                csv_analysis["positions_distribution"].items()
            )[:10]:
                print(f"   {position}: {count}")

            print(
                f"\nüéÆ GAMING VERTICALS ({len(csv_analysis['gaming_verticals'])})"
            )
            for vertical, count in list(
                csv_analysis["gaming_verticals"].items()
            )[:10]:
                print(f"   {vertical}: {count}")

            if csv_analysis["countries"]:
                print(f"\nüåç –¢–û–ü –°–¢–†–ê–ù–´ ({len(csv_analysis['countries'])})")
                for country, count in list(csv_analysis["countries"].items())[
                    :10
                ]:
                    print(f"   {country}: {count}")

        # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        print(f"\nüìã –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•")
        quality = csv_analysis["data_quality"]
        print(f"   –ü–æ–ª–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏: {quality['complete_profiles']:,}")
        print(f"   –ë–µ–∑ –ø–æ–∑–∏—Ü–∏–∏: {quality['missing_positions']:,}")
        print(f"   –ë–µ–∑ –∫–æ–º–ø–∞–Ω–∏–∏: {quality['missing_companies']:,}")
        print(
            f"   –ë–µ–∑ gaming vertical: {quality['missing_gaming_vertical']:,}"
        )

        contact_quality = csv_analysis["contact_quality"]
        print(f"   –° –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {contact_quality['with_phone']:,}")
        print(f"   –° email: {contact_quality['with_email']:,}")

        print("=" * 80)

        return "–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ"

    def export_target_audience(
        self, output_file: str = None, apply_filters: bool = True
    ) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π CSV"""
        if not self.csv_file.exists():
            return "CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"

        try:
            df = pd.read_csv(self.csv_file)
            campaign_log = self.load_campaign_log()
            contacted_users = set(campaign_log.get("contacted_users", []))

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–∞–º–ø–∞–Ω–∏–∏
            available = df[
                (df["connected"].isna()) | (df["connected"] != "Sent")
            ].copy()

            if contacted_users:
                available = available[
                    ~available["user_id"].isin(contacted_users)
                ]

            if apply_filters:
                # –§–∏–ª—å—Ç—Ä—ã –ø–æ gaming_vertical
                online_mask = available["gaming_vertical"].str.contains(
                    "|".join(ONLINE_GAMING_KEYWORDS), case=False, na=False
                )
                empty_gaming_mask = (
                    available["gaming_vertical"].isna()
                    | (available["gaming_vertical"] == "")
                    | (available["gaming_vertical"].str.strip() == "")
                )
                land_mask = available["gaming_vertical"].str.contains(
                    "|".join(EXCLUDED_GAMING_KEYWORDS), case=False, na=False
                )

                available = available[
                    (online_mask | empty_gaming_mask) & ~land_mask
                ]

            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            available = available.drop_duplicates(
                subset=["user_id"], keep="first"
            )
            available = available[
                available["user_id"].notna()
                & (available["user_id"] != "")
                & available["full_name"].notna()
                & (available["full_name"] != "")
            ]

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ, –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
            if "position" in available.columns:
                priority_mask = available["position"].str.contains(
                    "|".join(PRIORITY_POSITIONS), case=False, na=False
                )
                priority_contacts = available[priority_mask].copy()
                regular_contacts = available[~priority_mask].copy()

                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                priority_contacts["is_priority"] = True
                regular_contacts["is_priority"] = False

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–≤–µ—Ä—Ö—É
                available = pd.concat(
                    [priority_contacts, regular_contacts], ignore_index=True
                )

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            if not output_file:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = (
                    self.data_dir / f"target_audience_{timestamp}.csv"
                )
            else:
                output_file = Path(output_file)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            available.to_csv(output_file, index=False, encoding="utf-8")

            return f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(available)} –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –≤ {output_file}"

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {e}"

    def cleanup_duplicates(self, dry_run: bool = True) -> str:
        """–û—á–∏—â–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –æ—Å–Ω–æ–≤–Ω–æ–º CSV —Ñ–∞–π–ª–µ"""
        if not self.csv_file.exists():
            return "CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"

        try:
            df = pd.read_csv(self.csv_file)
            original_count = len(df)

            # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ user_id
            duplicates = df[df.duplicated(subset=["user_id"], keep=False)]
            unique_user_duplicates = duplicates["user_id"].nunique()

            if len(duplicates) == 0:
                return "–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

            print(
                f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è –∑–∞–ø–∏—Å–µ–π –¥–ª—è {unique_user_duplicates} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö user_id"
            )

            if dry_run:
                print("–†–ï–ñ–ò–ú –ü–†–û–°–ú–û–¢–†–ê - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
                print("\n–ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
                for user_id in duplicates["user_id"].unique()[:5]:
                    user_dupes = duplicates[duplicates["user_id"] == user_id]
                    print(f"\nUser ID: {user_id}")
                    for _, row in user_dupes.iterrows():
                        print(
                            f"  - {row['full_name']} | {row.get('company_name', 'N/A')} | {row.get('position', 'N/A')}"
                        )

                return f"–ù–∞–π–¥–µ–Ω–æ {len(duplicates)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å --no-dry-run –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è."

            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤—ã–π
            df_cleaned = df.drop_duplicates(subset=["user_id"], keep="first")
            removed_count = original_count - len(df_cleaned)

            # –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
            backup_file = self.csv_file.with_suffix(".backup.csv")
            df.to_csv(backup_file, index=False, encoding="utf-8")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            df_cleaned.to_csv(self.csv_file, index=False, encoding="utf-8")

            return f"–£–¥–∞–ª–µ–Ω–æ {removed_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {backup_file}"

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}"

    def check_campaign_readiness(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∑–∞–ø—É—Å–∫—É –∫–∞–º–ø–∞–Ω–∏–∏"""
        checks = {
            "csv_exists": self.csv_file.exists(),
            "csv_readable": False,
            "has_target_audience": False,
            "working_time": is_working_time(),
            "config_valid": False,
            "recommendations": [],
        }

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º CSV
            if checks["csv_exists"]:
                df = pd.read_csv(self.csv_file)
                checks["csv_readable"] = True

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é
                target_analysis = self.identify_target_audience()
                if "error" not in target_analysis:
                    checks["has_target_audience"] = (
                        target_analysis["total_available"] > 0
                    )

                    if target_analysis["total_available"] < 10:
                        checks["recommendations"].append(
                            "–ú–∞–ª–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –¥–ª—è –∫–∞–º–ø–∞–Ω–∏–∏"
                        )

                    if target_analysis["priority_contacts"] > 0:
                        checks["recommendations"].append(
                            f"–ù–∞–π–¥–µ–Ω–æ {target_analysis['priority_contacts']} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤"
                        )
            else:
                checks["recommendations"].append("CSV —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            try:
                validate_config()
                checks["config_valid"] = True
            except ValueError as e:
                checks["recommendations"].append(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è
            if not checks["working_time"]:
                checks["recommendations"].append(
                    "–°–µ–π—á–∞—Å –Ω–µ—Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è —Å–æ–≥–ª–∞—Å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º"
                )

            # –û–±—â–∞—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å
            checks["ready"] = all(
                [
                    checks["csv_exists"],
                    checks["csv_readable"],
                    checks["has_target_audience"],
                    checks["config_valid"],
                ]
            )

        except Exception as e:
            checks["recommendations"].append(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏: {e}")

        return checks


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI —É—Ç–∏–ª–∏—Ç—ã"""
    parser = argparse.ArgumentParser(description="Campaign Management Utility")

    subparsers = parser.add_subparsers(
        dest="command", help="–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã"
    )

    # –ö–æ–º–∞–Ω–¥–∞ –æ—Ç—á–µ—Ç–∞
    report_parser = subparsers.add_parser("report", help="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞")
    report_parser.add_argument(
        "--detailed", action="store_true", help="–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç"
    )

    # –ö–æ–º–∞–Ω–¥–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏
    export_parser = subparsers.add_parser(
        "export", help="–≠–∫—Å–ø–æ—Ä—Ç —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏"
    )
    export_parser.add_argument("--output", "-o", help="–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª")
    export_parser.add_argument(
        "--no-filters", action="store_true", help="–ë–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤"
    )

    # –ö–æ–º–∞–Ω–¥–∞ –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    cleanup_parser = subparsers.add_parser(
        "cleanup", help="–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"
    )
    cleanup_parser.add_argument(
        "--no-dry-run", action="store_true", help="–í—ã–ø–æ–ª–Ω–∏—Ç—å –æ—á–∏—Å—Ç–∫—É"
    )

    # –ö–æ–º–∞–Ω–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
    check_parser = subparsers.add_parser(
        "check", help="–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ –∫–∞–º–ø–∞–Ω–∏–∏"
    )

    # –ö–æ–º–∞–Ω–¥–∞ –∞–Ω–∞–ª–∏–∑–∞
    analyze_parser = subparsers.add_parser("analyze", help="–ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    analyze_parser.add_argument(
        "--target", action="store_true", help="–ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏"
    )

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    manager = CampaignManager()

    try:
        if args.command == "report":
            manager.generate_campaign_report(detailed=args.detailed)

        elif args.command == "export":
            result = manager.export_target_audience(
                output_file=args.output, apply_filters=not args.no_filters
            )
            print(result)

        elif args.command == "cleanup":
            result = manager.cleanup_duplicates(dry_run=not args.no_dry_run)
            print(result)

        elif args.command == "check":
            checks = manager.check_campaign_readiness()

            print("üîç –ü–†–û–í–ï–†–ö–ê –ì–û–¢–û–í–ù–û–°–¢–ò –ö –ö–ê–ú–ü–ê–ù–ò–ò")
            print("=" * 40)
            print(f"‚úÖ CSV —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {checks['csv_exists']}")
            print(f"‚úÖ CSV —á–∏—Ç–∞–µ—Ç—Å—è: {checks['csv_readable']}")
            print(
                f"‚úÖ –ï—Å—Ç—å —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è: {checks['has_target_audience']}"
            )
            print(f"‚úÖ –†–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è: {checks['working_time']}")
            print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω–∞: {checks['config_valid']}")
            print(f"\nüéØ –ì–û–¢–û–í–ù–û–°–¢–¨: {'–î–ê' if checks['ready'] else '–ù–ï–¢'}")

            if checks["recommendations"]:
                print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
                for rec in checks["recommendations"]:
                    print(f"   - {rec}")

        elif args.command == "analyze":
            if args.target:
                analysis = manager.identify_target_audience()
                if "error" in analysis:
                    print(f"‚ùå {analysis['error']}")
                else:
                    print("üéØ –ê–ù–ê–õ–ò–ó –¶–ï–õ–ï–í–û–ô –ê–£–î–ò–¢–û–†–ò–ò")
                    print("=" * 40)
                    print(
                        f"–î–æ—Å—Ç—É–ø–Ω–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {analysis['total_available']:,}"
                    )
                    print(f"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ: {analysis['priority_contacts']:,}")
                    print(f"–û–±—ã—á–Ω—ã–µ: {analysis['regular_contacts']:,}")
                    print(
                        f"–°–≤—è–∑–∞–ª–∏—Å—å —Ä–∞–Ω–µ–µ: {analysis['contacted_previously']:,}"
                    )

                    if analysis["gaming_verticals"]:
                        print(f"\nTO–ü Gaming Verticals:")
                        for vertical, count in list(
                            analysis["gaming_verticals"].items()
                        )[:5]:
                            print(f"   {vertical}: {count}")
            else:
                analysis = manager.analyze_csv_data()
                if "error" in analysis:
                    print(f"‚ùå {analysis['error']}")
                else:
                    print("üìä –ê–ù–ê–õ–ò–ó –ë–ê–ó–´ –î–ê–ù–ù–´–•")
                    print("=" * 40)
                    print(f"–í—Å–µ–≥–æ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤: {analysis['total_contacts']:,}")
                    print(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ: {analysis['contacted_count']:,}")
                    print(f"–° –æ—Ç–≤–µ—Ç–∞–º–∏: {analysis['with_responses']:,}")
                    print(f"–û–∂–∏–¥–∞—é—Ç: {analysis['pending_contacts']:,}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
